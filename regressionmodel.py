# -*- coding: utf-8 -*-
"""regressionModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NG_k4MyMqmOxUu5UODtiLBajm3GGTzLX

# Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

#1. GeoPandas is a project to add support for geographic data to pandas objects (work with spatial data)
!pip install geopandas

#2. encode categorical data
#!pip install category-encoders

#3. Local Interpretable Model-Agnostic Explanations for machine learning classifiers
#!pip install lime

!pip install mapclassify

## for data
import pandas as pd
import numpy as np

import pandas.util.testing as tm

## for plotting
import matplotlib.pyplot as plt
import seaborn as sns

## for statistical tests
import scipy
import statsmodels.formula.api as smf
import statsmodels.api as sm

## for machine learning
from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition

## for explainer
#from lime import lime_tabular

"""# Data Preparation

## 1 Load Dataset
"""

# Define relative path to file
EDAsurvey = pd.read_csv('/content/drive/My Drive/EDA/EDAsurvey.csv')
EDAsurvey.head()

"""## 2 Cleaning: drop RCODE and DESCRIPT"""

# Remove column  RCODE and DESCRIPT for ML
EDAsurvey.drop(['RCODE', 'DESCRIPT'], inplace = True, axis = 1) 
EDAsurvey.head()

EDAsurvey.info()

"""## 3 Spatial Analysis

### 3.0 Convert PandasDataFrame to GeoDatrame
"""

import geopandas as gpd

#convert PandasDataFrame to GeoDataFrame (needs shapely object)
#We use geopandas points_from_xy() to transform x=Longitude and y=Latitude into a list of shapely
EDAsurvey_point = gpd.GeoDataFrame(EDAsurvey, geometry=gpd.points_from_xy(EDAsurvey.x, EDAsurvey.y))

EDAsurvey_point.head()

#plot points cloud
EDAsurvey_point.plot()

"""### 3.1 Map projection"""

from pyproj import CRS

# option 2: Defining projection: we assign the GDA94 / MGA zone 55 latitude-longitude CRS (EPSG:28355) to the crs attribute:
EDAsurvey_point.crs = CRS("EPSG:28355")
EDAsurvey_point.crs

"""### 3.2 Plot point clouds"""

import matplotlib.pyplot as plt

# Make subplots that are next to each other
fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 8))

# Plot the data in WGS84 CRS
EDAsurvey_point.plot(ax=ax1, color='#ff48a5', edgecolor='#ffcae5', label="Points cloud");

# Add title
ax1.set_title("GDA94 / MGA zone 55");
ax1.legend(title="Tree's Location",loc='upper center', bbox_to_anchor=(1.1, 0.8), frameon=False)

# Remove empty white space around the plot
plt.tight_layout()

"""### 3.3 Write as ESRI Shape File"""

from google.colab import files

EDAsurvey_point.to_file("EDAsurvey_point.shp")

#dowload into local machine
#files.download("EDAsurvey_point.shp")

"""### 3.4 Map Spatial Distribution of site index

### 3.4.1 Map option 1
"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib import cm
from mpl_toolkits.axes_grid1 import make_axes_locatable
import mapclassify

# 1. MAP 1
## Plot Soil C
# set the range for the choropleth
vmin, vmax = 4, 51

# create figure and axes for Matplotlib
fig, ax = plt.subplots(1, figsize=(6, 7), subplot_kw={'aspect':'equal'})

# Point data
EDAsurvey_point.plot(column='siteindex', scheme='Quantiles', k =7, cmap='Spectral', alpha=1, markersize=30, ax=ax)

# add a title
ax.set_title('Site Index Productivity (m)', fontdict={'fontsize': '20', 'fontweight' : '3'})

# create an axes on the right side of ax. The width of cax will be 5%
# of ax and the padding between cax and ax will be fixed at 0.05 inch.
# You need to import mpl_toolkits.axes_grid1 
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.05)

# Create colorbar as a legend
sm = plt.cm.ScalarMappable(cmap='Spectral',
                           norm=plt.Normalize(vmin=vmin, vmax=vmax))

# empty array for the data range
sm._A = []

# add the colorbar to the figure
cbar = fig.colorbar(sm, cax=cax)

"""### 3.4.2 Map option 2"""

# 2. MAP 2
fig, ax = plt.subplots(figsize=(14,12), subplot_kw={'aspect':'equal'})

EDAsurvey_point.plot(column="siteindex", scheme='Quantiles', k=5, cmap='Spectral', legend=True, legend_kwds={'title':'Growth Potential at a Site:'}, ax=ax)

# add a title for the plot
ax.set_title("Productivity in Radiata pine \n Quantiles Measurement of the Growth Potential at a Site");

plt.savefig('pointsRadiataTas.jpg', bbox_inches='tight', dpi=300)

"""## 4 Save new dataframe with geometry points"""

from google.colab import  drive

#Mounts the google drive to Colab Notebook
drive.mount('/content/drive', force_remount=True)

#Make sure the folder Name is created in the google drive before uploading
EDAsurvey_point.to_csv('/content/drive/My Drive/EDAsurvey_point/EDAsurvey_point.csv', index = False, header=True)

# Define relative path to file
EDAsurvey_point = pd.read_csv('/content/drive/My Drive/EDAsurvey_point/EDAsurvey_point.csv')
EDAsurvey_point.head()

from sklearn import preprocessing
# Get column names first
names = EDAsurvey.columns

# Create the Scaler object
scaler2 = preprocessing.MinMaxScaler()

# Fit your data on the scaler object
EDAsurvey_norm = scaler2.fit_transform(EDAsurvey)
EDAsurvey_norm = pd.DataFrame(EDAsurvey_norm, columns=names)

sns.set(style="whitegrid")

x = "siteindex"

fig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)
fig.suptitle(x, fontsize=20)
fig.set_size_inches(8, 4)

### distribution
ax[0].title.set_text('distribution')
variable = EDAsurvey[x].fillna(EDAsurvey[x].mean())
breaks = np.quantile(variable, q=np.linspace(0, 1, 11))
variable = variable[ (variable > breaks[0]) & (variable < 
                    breaks[10]) ]

sns.distplot(variable, hist=True, kde=True, kde_kws={"shade": True}, ax=ax[0])
des = EDAsurvey[x].describe()
ax[0].axvline(des["25%"], ls='--')
ax[0].axvline(des["mean"], ls='--')
ax[0].axvline(des["75%"], ls='--')
ax[0].grid(True)
des = round(des, 2).apply(lambda x: str(x))
box = '\n'.join(("min: "+des["min"], "25%: "+des["25%"], "mean: "+des["mean"], "75%: "+des["75%"], "max: "+des["max"]))
ax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha="right", bbox=dict(boxstyle='round', facecolor='white', alpha=1))

### boxplot 
ax[1].title.set_text('outliers (norm scale)')
tmp_dtf = pd.DataFrame(EDAsurvey_norm[x])
#tmp_dtf[x] = np.log(EDAsamplewcML_normalize[x])
tmp_dtf.boxplot(column=x, ax=ax[1])
plt.show()

### boxplot 
#ax[1].title.set_text('outliers (log scale)')
#tmp_dtf = pd.DataFrame(EDAsamplewcML[x])
#tmp_dtf[x] = np.log(EDAsamplewcML[x])
#tmp_dtf.boxplot(column=x, ax=ax[1])
#plt.show()

"""# PRE-PROCESSING

# 1 Categorical Data

## 1.1 Target Encoding
"""

# 1. OPTION 1
def calc_mean(EDAsurvey, by, on, m):
    # Compute the global mean
    mean = EDAsurvey[on].mean()

    # Compute the number of values and the mean of each group
    agg = EDAsurvey.groupby(by)[on].agg(['count', 'mean'])
    counts = agg['count']
    means = agg['mean']

    # Compute the "smoothed" means
    TargEnc = (counts * means) / (counts)

    # Replace each value by the according smoothed mean
    return EDAsurvey[by].map(TargEnc)

EDAsurvey.loc[:, 'SYMBOL'] = calc_mean(EDAsurvey, by='SYMBOL', on='siteindex', m=0)
EDAsurvey.loc[:, 'soil_order'] = calc_mean(EDAsurvey, by='soil_order', on='siteindex', m=0)
EDAsurvey.head()

"""## 1.2 For Descriptive Statistics

### a) Extract categorical values encoded
"""

# soil_order values
mean = EDAsurvey.siteindex.mean()
agg = EDAsurvey.groupby('soil_order')['siteindex'].agg(['count', 'mean'])
counts = agg['count']
means = agg['mean']

smoothSoil = (counts * means + 5 * mean) / (counts + 5)
smoothSoil

# soil_order values
mean = EDAsurvey.siteindex.mean()
agg = EDAsurvey.groupby('soil_order')['siteindex'].agg(['count', 'mean'])
counts = agg['count']
means = agg['mean']

Soil = (counts * means) / (counts)
Soil

# sort values ascending
smoothSoil.sort_values()

# geology SYMBOL values
mean = EDAsurvey.siteindex.mean()
agg = EDAsurvey.groupby('SYMBOL')['siteindex'].agg(['count', 'mean'])
counts = agg['count']
means = agg['mean']

smoothGeol = (counts * means + 29 * mean) / (counts + 29)
smoothGeol

# geology SYMBOL values
mean = EDAsurvey.siteindex.mean()
agg = EDAsurvey.groupby('SYMBOL')['siteindex'].agg(['count', 'mean'])
counts = agg['count']
means = agg['mean']

Geol = (counts * means + 0 * mean) / (counts + 0)
Geol

# sort GEOLOGY values ascending
smoothGeol.sort_values()

"""### b.1) Plot siteindex group by soil classification"""

x, y = "soil_order", "siteindex"
plt.style.use('classic')

### bin plot
EDAsurvey_noNan = EDAsurvey[EDAsurvey[x].notnull()]
breaks = np.quantile(EDAsurvey_noNan[x], q=np.linspace(0, 1, 11))
groups = EDAsurvey_noNan.groupby([pd.cut(EDAsurvey_noNan[x], bins=breaks, 
           duplicates='drop')])[y].agg(['mean','median','size'])

fig, ax = plt.subplots()
#fig, ax = plt.subplots(nrows=2, ncols=2,  sharex=False, sharey=False)
#fig.suptitle(x+"   vs   "+y, fontsize=20)
fig.set_size_inches(8, 6)

fig.suptitle(x+"   vs   "+y, fontsize=10)
groups[["mean", "median"]].plot(kind="line", ax=ax)
groups["size"].plot(kind="bar", ax=ax, rot=45, secondary_y=True,
                    color="grey", alpha=0.3, grid=True)
ax.set(ylabel=y)
ax.right_ax.set_ylabel("Observations in each bin")
plt.setp(ax.get_xticklabels(), rotation=30, ha='right')

### scatter plot
#sns.jointplot(x=x, y=y, data=EDAsurvey, dropna=True, kind='reg')
plt.savefig('SOILvsSITE.png', bbox_inches='tight', dpi=300)

x, y = "soil_order", "siteindex"

### bin plot
EDAsurvey_noNan = EDAsurvey[EDAsurvey[x].notnull()]
breaks = np.quantile(EDAsurvey_noNan[x], q=np.linspace(0, 1, 11))
groups = EDAsurvey_noNan.groupby([pd.cut(EDAsurvey_noNan[x], bins=breaks, 
           duplicates='drop')])[y].agg(['mean','median','size'])

fig, ax = plt.subplots()
#fig, ax = plt.subplots(nrows=2, ncols=2,  sharex=False, sharey=False)
#fig.suptitle(x+"   vs   "+y, fontsize=20)
fig.set_size_inches(8, 6)

fig.suptitle(x+"   vs   "+y, fontsize=20)
groups[["mean", "median"]].plot(kind="line", ax=ax)
groups["size"].plot(kind="bar", ax=ax, rot=45, secondary_y=True,
                    color="grey", alpha=0.3, grid=True)
ax.set(ylabel=y)
ax.right_ax.set_ylabel("Observations in each bin")
plt.setp(ax.get_xticklabels(), rotation=30, ha='right')

"""### b.2) Plot siteindex group by geology structure"""

x, y = "SYMBOL", "siteindex"
plt.style.use('classic')
### bin plot
EDAsurvey_noNan = EDAsurvey[EDAsurvey[x].notnull()]
breaks = np.quantile(EDAsurvey_noNan[x], q=np.linspace(0, 1, 11))
groups = EDAsurvey_noNan.groupby([pd.cut(EDAsurvey_noNan[x], bins=breaks, 
           duplicates='drop')])[y].agg(['mean','median','size'])

fig, ax = plt.subplots()
#fig, ax = plt.subplots(nrows=2, ncols=2,  sharex=False, sharey=False)
#fig.suptitle(x+"   vs   "+y, fontsize=20)
fig.set_size_inches(8, 6)

fig.suptitle(x+"   vs   "+y, fontsize=10)
groups[["mean", "median"]].plot(kind="line", ax=ax)
groups["size"].plot(kind="bar", ax=ax, rot=45, secondary_y=True,
                    color="grey", alpha=0.3, grid=True)
ax.set(ylabel=y)
ax.right_ax.set_ylabel("Observations in each bin")
plt.setp(ax.get_xticklabels(), rotation=30, ha='right')

### scatter plot
#sns.jointplot(x=x, y=y, data=EDAsurvey, dropna=True, kind='reg')
plt.savefig('GEOLvsSITE.png', bbox_inches='tight', dpi=300)

import statistics
from scipy import stats

# Use stats. function to describe the mean of observations
stats.describe(EDAsurvey.SYMBOL)

"""## 2 Features selection?

### 2.1 Correlation Analysis: Spearman
"""

# 1.create a copy of dataframe
EDAsurveyCORR = EDAsurvey.copy()

# Remove column  RCODE and DESCRIPT for ML
#EDAsurveyCORR.drop(['x', 'y'], inplace = True, axis = 1) 
EDAsurveyCORR.head()

# 2.correlation with Spearman
spearmancorr = EDAsurveyCORR.corr(method='spearman')
spearmancorr

# 3.correlation with Spearman Improved
plt.style.use('seaborn-whitegrid')
plt.figure(figsize=(16,8))

corr_matrix = EDAsurveyCORR.corr(method="spearman")

sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)
plt.title("Spearman correlation")
#Rotate labels x-axis
plt.xticks(rotation=45, horizontalalignment='right')

plt.savefig('Spearmancorr_EDAsurvey.png', bbox_inches='tight', dpi=300)

"""## 2.2 Drop features high correlated"""

# Remove 4 columns: less feature importance
EDAsurvey.drop(columns=['catchmentArea_SAGA', 'SVF_simplified', 'Gh_total', 'distFromCoast'], inplace= True, axis = 1) 
#EDAsurvey.head()

#EDAsurvey.shape
#OUTPUT: (953556, 24)

"""# 3 Normalize Data"""

#NORMALIZE
from sklearn.preprocessing import MinMaxScaler

y = EDAsurveyCORR['siteindex']
EDAsurveyCORR = EDAsurveyCORR.loc[:, ~EDAsurveyCORR.columns.isin(['siteindex'])]

scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(EDAsurveyCORR)
EDAsurveyCORR.loc[:,:] = scaled_values

EDAsurveyCORR['siteindex'] = y

EDAsurveyCORR.head()

"""## Boxplot

### Option a)
"""

# drop coordinates
EDAsurveyCORR.drop(columns=['x', 'y'], inplace= True, axis = 1)

columns = list(EDAsurveyCORR.columns)
new_column_names = []
for col in columns:
    new_column_names.append(col.replace(' ', '_'))
EDAsurveyCORR.columns = new_column_names

import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

plt.figure(figsize=(10,12))
sns.boxplot(data=EDAsurveyCORR.drop(columns=['siteindex']), orient='horizontal', palette='husl')

plt.savefig('featureSelectBoxplot.png', bbox_inches='tight', dpi=300)

"""### Option b)"""

from sklearn import preprocessing
import numpy as np
# Get column names first
names = EDAsurveyCORR.columns

# Create the Scaler object
scaler2 = preprocessing.MinMaxScaler()

# Fit your data on the scaler object
EDAsurvey_normalize = scaler2.fit_transform(EDAsurveyCORR)
EDAsurvey_normalize = pd.DataFrame(EDAsurvey_normalize, columns=names)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#sns.set_style("white")

plt.style.use('classic')

# Seaborn visualization library
medianprops = dict(linewidth=1.5, linestyle='-', color='#fc3468')
meanprops =  dict(marker='D', markerfacecolor='indianred', markersize=4.5)
flierprops = dict(marker='o', markerfacecolor='#7fcdbb', markersize=3, alpha=0.5)

box = EDAsurvey_normalize.boxplot(return_type='axes', figsize=(10,12), vert=False, medianprops=medianprops, 
                                  meanprops=meanprops, flierprops = flierprops,
                                  notch=False,  showmeans=True)
                           
# Show the grid lines as light grey lines
plt.grid(color='white', linestyle='-', linewidth=0.25, alpha=0.5)
plt.gca().spines['left'].set_color('none')
plt.gca().spines['right'].set_color('none')

plt.savefig('boxplot_EDAsurvey_norm.png', bbox_inches='tight', dpi=300)



"""## Correlation between features"""

#check correlation after drop 4 features
corr_matrix = EDAsurveyCORR.corr().abs()
plt.figure(figsize=(16,10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

"""# LINEAR MODEL

# 4 Split into training

## 4.1 Split into training: test=10%
"""

# A_Target variable: Labels are the values we want to predict
X = EDAsurvey.drop('siteindex', axis = 1)

# Saving feature names for later use
X_list = list(EDAsurvey.columns)

# B_Independent variables: features are the values that help to predict
y = EDAsurvey['siteindex']#.values.reshape(-1,1)

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)

print('Training Features Shape:', X_train.shape)
print('Training Labels Shape:', X_test.shape)
print('Testing Features Shape:', y_train.shape)
print('Testing Labels Shape:', y_test.shape)

X_train

EDAsurvey.head()

"""## 4.2 Extract train and test idx for later merge with geography coord"""

# Extracting train and test idx for later merge with additional data or geography coordinates
test_idx=np.asarray(X_test.index)
train_idx=np.asarray(X_train.index)

X_test_coord=EDAsurvey[[ 'x', 'y', 'siteindex']].iloc[test_idx]
X_test_coord.reset_index(inplace=True,drop=True)

#test_idx

X_test.shape
#output: (95356, 25)

y_train.reset_index(drop=True,inplace=True)
y_test.reset_index(drop=True,inplace=True)

X_train.reset_index(drop=True,inplace=True)
X_test.reset_index(drop=True,inplace=True)

# drop coordinates
EDAsurvey.drop(columns=['x', 'y'], inplace= True, axis = 1)

EDAsurvey.head()

"""# 5 Fit: Linear Regression

## 5.1 Linear Regression Model | Ordinary Least Squares Method
"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn import linear_model

# Create a linear regression object
model = linear_model.LinearRegression()

# TRAIN: Fit the model using the training set
model.fit(X_train, y_train)

"""## 5.2 Predict Test Results"""

# TEST: Make prediction using test set
predictedStand = model.predict(X_test)
predictedStand

dataTest = pd.DataFrame({'Actual': y_test, 'Predicted': predictedStand})
dataTest['residuals']=dataTest['Actual'] - dataTest['Predicted']
dataTest

dataTest.describe()

# TRAIN: Make prediction using TRAIN set
y_train_predicted = model.predict(X_train)
y_train_predicted

dataTrain = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_predicted})
dataTrain['residuals']=dataTrain['Actual'] - dataTrain['Predicted']
dataTrain

dataTrain.describe()

"""### 5.2.1 Plot Predicted vs Observed | Test Set"""

import numpy as np   # To perform calculations
import matplotlib.pyplot as plt  # To visualize data and regression line
from pylab import rcParams
import seaborn as sns

sns.set(style="whitegrid")

dfTest = dataTest.head(25)
dfTest.plot(kind='bar', figsize=(12,6))

#plt.legend(title="Train set",loc='upper center', bbox_to_anchor=(1.10, 0.8), frameon=False)
plt.legend(title="Train set", frameon=  True)
plt.title('Actual vs Predicted \'siteindex\' Values in Train Set' )
plt.grid(which='major', linestyle='-', linewidth='0.5', color='grey')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.xticks(rotation=45, horizontalalignment='right')

plt.savefig('actualvsPredictedBar_LM_testSet.jpg', bbox_inches='tight', dpi=300)

import matplotlib.pyplot as plt
import seaborn as sns
#sns.set(style='whitegrid')

plt.style.use('seaborn-whitegrid')

plt.figure(figsize=(10, 6))

ax = sns.regplot(x="Actual", y="Predicted", data=dataTest, label='siteindex predicted', scatter_kws = {'color': 'white', 'alpha': 0.8, 'edgecolor':'blue', 's':10}, line_kws = {'color': '#f54a19'})
ax.set_ylim(0,55)
ax.set_xlim(0,55)
ax.plot([0, 55], [0, 55], 'k--', lw=2)

ax.legend(title="Test set:", frameon=  True, loc='upper left')
#ax.legend(bbox_to_anchor =(0.85, -0.20), ncol = 4) 
plt.title('Goodness-of-fit in Validation Set',fontsize=12)

plt.savefig('actualvsPredicted_LM_testSet.jpg', bbox_inches='tight', dpi=300)

"""## 5.3 Perfomance and Validation"""

# ACCURACY FOR TRAINING SET:
print("Accuracy on training set: {:.3f}".format(model.score(X_train, y_train)))

# ACCURACY FOR TEST SET:
print("Accuracy on test set: {:.3f}".format(model.score(X_test, y_test)))

print("R2 (explained variance) Train Set: {:.3f}".format(metrics.r2_score(y_train, y_train_predicted), 2))
print("R2 (explained variance) Test set: {:.3f}".format(metrics.r2_score(y_test, predictedStand), 2))
print('MAE=Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictedStand))  
print('MSE=Mean Squared Error:', metrics.mean_squared_error(y_test, predictedStand))  
print('RMSE=Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictedStand)))

from math import sqrt
interval = 1.96 * sqrt( (0.488 * (1 - 0.488)) / 95356)
print('%.3f' % interval)

"""### 5.3.1 Plot Squared Errror vs Observed """

residSquare = np.square(dataTest['residuals'])
residSquare

import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
fig=plt.figure(figsize = [8, 6])

ax = fig.add_subplot(111)

ax.scatter(x=dataTest['Actual'], y=residSquare, label='Squared Error', c='white', alpha=0.8, edgecolors='#1b346c', s=10)
ax.set_xlabel("Observed 'site index' values") #it's a good idea to label your axes
ax.set_ylabel('Squared Error')



plt.title("Squared Error vs Observed 'site index' values")
plt.legend(title="",loc='upper right', frameon=True)
plt.savefig('SquaredError_LM.png', bbox_inches='tight', dpi=300)

fig=plt.figure(figsize = [8, 6])

ax = fig.add_subplot(111)

ax.scatter(x=dataTest['Predicted'], y=residSquare, c='#f54a19', label='Squared Error')
ax.set_xlabel("Predicted 'site index' values") #it's a good idea to label your axes
ax.set_ylabel('Squared Error')

plt.title("Squared Error vs Predicted 'site index' values")
plt.legend(title="",loc='upper right', frameon=True)
plt.savefig('SquaredErrorPredicted_LM.png', bbox_inches='tight', dpi=300)

"""## 5.4 Evaluation: Slope of Coefficients"""

from sklearn.metrics import mean_squared_error, r2_score
# Model Output
# a. Intercept
print("Intercept:", model.intercept_)

# b. Coefficient - the slop of the line
print("Coefficients(slope of the line):", model.coef_)

# c. the error - the mean square error
print("Mean squared error: %.3f"% mean_squared_error(y_test,predictedStand))

# d. R-square -  how well x accout for the varaince of Y
print("R-square: %.3f'" % r2_score(y_test,predictedStand))

#1.Run split into training
#y = EDAsurvey['siteindex'].values.reshape(-1,1)
#2. run model again

pred_model = pd.DataFrame(['aspect','planCurvature','profileCurvature','slope','TPI','TWI_SAGA','Dh_diffuse','Ih_direct','DEM','meanJanRain','meanJulRain','maxJanTemp','minJulTemp','SYMBOL','soil_order','BDw','CLY','CFG','ECD','SOC','pHw','SND','SLT'])
coeff = pd.DataFrame(model.coef_, index=['Co-efficient']).transpose()

pd.concat([pred_model,coeff], axis=1, join='inner')

"""### 5.4.1 Plot Slopes"""

column_names = ['aspect','planCurvature', 'profileCurvature','slope','TPI','TWI_SAGA','Dh_diffuse','Ih_direct','DEM','meanJanRain','meanJulRain','maxJanTemp','minJulTemp','SYMBOL','soil_order','BDw','CLY', 'CFG','ECD','SOC','pHw','SND','SLT']

regression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': model.coef_}, columns=['Feature', 'Coefficient'])

plt.figure(figsize=(14,8))
g = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')
g.set_title("Contribution of features towards dependent variable: 'siteindex' (y)", fontsize=15)
g.set_xlabel("independent variables (x)", fontsize=13)
g.set_ylabel("slope of coefficients (m)", fontsize=13)
plt.xticks(rotation=45, horizontalalignment='right')
g.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])
g.set_xticklabels(column_names)
for p in g.patches:
    g.annotate(np.round(p.get_height(),decimals=2), 
                (p.get_x()+p.get_width()/2., p.get_height()), 
                ha='center', va='center', xytext=(0, 10), 
               textcoords='offset points', fontsize=14, color='black')
    
plt.grid(which='major', linestyle='-', linewidth='0.5', color='grey')
#plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')


plt.savefig('FI_LM.png', bbox_inches='tight', dpi=300)

"""## 5.5 Mapping Predictions"""

df_coord=pd.concat([dataTest,X_test_coord], axis=1)
df_coord

"""### a) Convert to SHAPEFILE"""

import geopandas as gpd
from pyproj import CRS

# create data-frame
grid_df_coord = df_coord[['Actual','Predicted','residuals','x','y','siteindex']]

# Convert to spatial point data-frame
grid_df_coord = gpd.GeoDataFrame(
    grid_df_coord, geometry=gpd.points_from_xy(grid_df_coord.x, grid_df_coord.y))

grid_df_coord.crs = CRS("EPSG:28355")
grid_df_coord.crs

#Save as ESRI shape file
from google.colab import files

grid_df_coord.to_file("ML_PRED_siteindex.shp")

"""### b) Convert to raster"""

# Input  point shape file
ml_point = 'ML_PRED_siteindex.shp'

# Output raster
ml_output = 'ML_PRED_siteindex.tif'

import gdal
from osgeo import ogr, gdal
from osgeo import gdalconst

# Reference raster
point_ndsm = 'fiveSiteIndex.tif'
point_data = gdal.Open(point_ndsm, gdalconst.GA_ReadOnly)

# Get geo-information
geo_transform = point_data.GetGeoTransform()
x_min = geo_transform[0]
y_max = geo_transform[3]
x_max = x_min + geo_transform[1] * point_data.RasterXSize
y_min = y_max + geo_transform[5] * point_data.RasterYSize
x_res = point_data.RasterXSize
y_res = point_data.RasterYSize
mb_v = ogr.Open(ml_point) # change 
mb_l = mb_v.GetLayer()
pixel_width = geo_transform[1]

# Raster coversion
target_ds = gdal.GetDriverByName('GTiff').Create(ml_output, x_res, y_res, 1, gdal.GDT_Float32) # Change
target_ds.SetGeoTransform((x_min, pixel_width, 0, y_min, 0, pixel_width))
band = target_ds.GetRasterBand(1)
band.FlushCache()
gdal.RasterizeLayer(target_ds, [1], mb_l, options=["ATTRIBUTE=Predicted"]) # Change 

target_ds = None

"""### c) Plot Shapefile created"""

#Open GP=state polygon data 

import geopandas as gpd

fp_state="ML_PRED_siteindex.shp"

# read file using gpd.read_file()
ML_PRED_siteindex = gpd.read_file(fp_state) 
ML_PRED_siteindex.head()
#ML_PRED_siteindex= gpd.GeoDataFrame.from_file(fp_state)
#ML_PRED_siteindex.geom_type.head()

fp_state = ('/content/drive/My Drive/EDAsurvey_point/ML_PRED_siteindex')

import geopandas as gpd
# read file using gpd.read_file()
ML_PRED_siteindex = gpd.read_file(fp_state) 
ML_PRED_siteindex.head()

from pyproj import CRS

# Defining projection: we assign the GDA94 / MGA zone 55 latitude-longitude CRS (EPSG:28355) to the crs attribute:
#state_BD.crs = CRS("EPSG:28355")
ML_PRED_siteindex.crs

ML_PRED_siteindex.plot()

"""### d) Analysis Shapefile: Test set"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib import cm
from mpl_toolkits.axes_grid1 import make_axes_locatable
import mapclassify
# 2. MAP 2
fig, ax = plt.subplots(figsize=(14,12), subplot_kw={'aspect':'equal'})

ML_PRED_siteindex.plot(column="Predicted", scheme='Quantiles', k=5, cmap='Spectral', legend=True, legend_kwds={'title':'Growth Potential at a Site:'}, ax=ax)

# add a title for the plot
ax.set_title("Productivity in Radiata pine | Test set in ML \n Quantiles Measurement of Predicted 'siteindex' Values");

plt.savefig('pointsRadiataTas_Predicted.jpg', bbox_inches='tight', dpi=300)

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib import cm
from mpl_toolkits.axes_grid1 import make_axes_locatable
import mapclassify
# 2. MAP 2
fig, ax = plt.subplots(figsize=(14,12), subplot_kw={'aspect':'equal'})

ML_PRED_siteindex.plot(column="Actual", scheme='Quantiles', k=5, cmap='Spectral', legend=True, legend_kwds={'title':'Growth Potential at a Site:'}, ax=ax)

# add a title for the plot
ax.set_title("Productivity in Radiata pine | Test set in ML \n Quantiles Measurement of Observed 'siteindex' Values");

plt.savefig('pointsRadiataTas_Actual.jpg', bbox_inches='tight', dpi=300)

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib import cm
from mpl_toolkits.axes_grid1 import make_axes_locatable
import mapclassify
# 2. MAP 2
fig, ax = plt.subplots(figsize=(14,12), subplot_kw={'aspect':'equal'})

ML_PRED_siteindex.plot(column="residuals", scheme='Quantiles', k=5, cmap='Spectral', legend=True, legend_kwds={'title':'Growth Potential at a Site:'}, ax=ax)

# add a title for the plot
ax.set_title("Productivity in Radiata pine | Test set in ML \n Quantiles Measurement of Residuals 'siteindex' Values");

plt.savefig('pointsRadiataTas_Residuals.jpg', bbox_inches='tight', dpi=300)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure()
ax = plt.axes(projection='3d')

import plotly.express as px

fig = px.scatter_3d(ML_PRED_siteindex, x='x', y='y', z=ML_PRED_siteindex.Predicted,
                    color=ML_PRED_siteindex.Predicted,
                    color_continuous_scale='hsv',
                    opacity=0.7)

# tight layout
fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))

fig.show()

#fig, ax = plt.subplots(nrows=1, ncols=1,  sharex=False, sharey=False)
plt.figure(figsize=(10,6))
sns.set(style="whitegrid")
plt.scatter(ML_PRED_siteindex.x, ML_PRED_siteindex.y, c=ML_PRED_siteindex.Predicted, cmap='hsv')

#ax.set_xlabel('Longitude') #it's a good idea to label your axes
#ax.set_ylabel('Latitude')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

cbar= plt.colorbar()
cbar.set_label('Site Index (m)', labelpad=+1)

plt.title("Site Index Predicted by Location")
#plt.legend(title="Site Index (m)",loc='upper center', bbox_to_anchor=(1.15, 0.8), frameon=False)

plt.savefig('PredictedTestSet2D.png', bbox_inches='tight', dpi=300)

from PIL import Image, ImageDraw

# library
import seaborn as sns

fig = plt.figure(figsize=(10,6))
ax = plt.axes(projection='3d')
sns.set(style="whitegrid")

# Data for a three-dimensional line
xline = ML_PRED_siteindex.x
yline = ML_PRED_siteindex.y
zline = ML_PRED_siteindex.Predicted

im = ax.scatter3D(xline, yline, zline, c= ML_PRED_siteindex.Predicted, cmap='hsv')

ax.set_xlabel('Longitude') #it's a good idea to label your axes
ax.set_ylabel('Latitude')

#cax = fig.add_axes([0.27, 0.8, 0.5, 0.05])

cbar= fig.colorbar(im, orientation='vertical', shrink=0.6)
cbar.set_label('Site Index (m)', labelpad=+1)

plt.title("Site Index Predicted by Location")
#plt.legend(title="Site Index (m)",loc='upper center', bbox_to_anchor=(1.15, 0.8), frameon=False)

plt.savefig('PredictedTestSet3D.png', bbox_inches='tight', dpi=300)

import seaborn as sns

fig = plt.figure(figsize=(10,6))
ax = plt.axes(projection='3d')
sns.set(style="whitegrid")

# Data for a three-dimensional line
xline = ML_PRED_siteindex.x
yline = ML_PRED_siteindex.y
zline = ML_PRED_siteindex.Predicted

im = ax.scatter3D(xline, yline, zline, c= ML_PRED_siteindex.Predicted, cmap='hsv')

ax.set_xlabel('Longitude') #it's a good idea to label your axes
ax.set_ylabel('Latitude')

#cax = fig.add_axes([0.27, 0.8, 0.5, 0.05])

cbar= fig.colorbar(im, orientation='vertical', shrink=0.6)
cbar.set_label('Site Index (m)', labelpad=+1)

plt.title("Site Index Predicted by Location")
#plt.legend(title="Site Index (m)",loc='upper center', bbox_to_anchor=(1.15, 0.8), frameon=False)

# Rotate it
ax.view_init(30, 245)
plt.show()

fig = plt.figure(figsize=(20,8))

ax = fig.add_subplot(111, projection='3d')

ax.scatter(ML_PRED_siteindex.x[0:10000], ML_PRED_siteindex.y[0:10000], ML_PRED_siteindex.Predicted[0:10000], c= ML_PRED_siteindex.Predicted[0:10000], cmap='viridis')
ax.view_init(50, 50)

ax.axis('tight')

#!pip install rasterio

"""### e) Plot raster: five site index"""

#!pip install earthpy
import os
import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep

import rasterio
import numpy as np
import matplotlib.pyplot as plt
from rasterio.merge import merge
from rasterio.plot import show

dtm_path = rasterio.open('fiveSiteIndex.tif')

dtm_path.meta

spatial_extent = rasterio.plot.plotting_extent(dtm_path)
spatial_extent

import seaborn as sns
sns.set(style='whitegrid')

fig, ax = plt.subplots(figsize=(24,20))

im = ax.imshow(data_array, cmap='terrain', extent=spatial_extent)

cb = fig.colorbar(im, orientation='vertical', shrink=0.2)
cb.set_label('elevation (m)', fontsize= 14)
ax.set_title("Digital Elevation Model of Five Site Index for Analisis")

"""### f) Plot raster: ML_PRED_siteindex

"""

from osgeo import ogr, gdal
from osgeo import gdalconst
import numpy as np

filename = "ML_PRED_siteindex.tif"
gdal_data = gdal.Open(filename)
gdal_band = gdal_data.GetRasterBand(1)
nodataval = gdal_band.GetNoDataValue()

# convert to a numpy array
data_array = gdal_data.ReadAsArray().astype(np.float)
data_array

# replace missing values if necessary
if np.any(data_array == nodataval):
    data_array[data_array == nodataval] = np.nan
    data_array[data_array == nodataval] = np.nan
    
data_array[data_array < 0] = np.nan
data_array[data_array >= 600] = np.nan

import seaborn as sns
sns.set(style='whitegrid')

fig, ax = plt.subplots(figsize=(24,20))

im = ax.imshow(data_array, cmap='terrain', extent=spatial_extent)

cb = fig.colorbar(im, orientation='vertical', shrink=0.2)
cb.set_label('elevation (m)', fontsize= 14)
ax.set_title("Test set for Analisis")

# outlier values in the data
print("the minimum raster value is: ", data_array.min())
print("the maximum raster value is: ", data_array.max())

"""## 5.6 Regression assumptions"""

error = dataTest['Actual'] - dataTest['Predicted']
#error = y_test - predictedStand
#error_info = pd.DataFrame({'y_true': y_test, 'y_pred': predictedStand, 'error': error}, columns=['y_true', 'y_pred', 'error'])

error_info = pd.DataFrame({'y_true': dataTest['Actual'], 'y_pred': dataTest['Predicted'], 'error': error}, columns=['y_true', 'y_pred', 'error'])

plt.figure(figsize = [6, 4]) # larger figure size for subplots

# Density Plot and Histogram of all A results
plt.subplot(1, 1, 1) # 1 row, 2 cols, subplot 1
sns.distplot(error_info.error, hist=True, kde=True, 
             bins=int(180/10), color = '#5f90d8', 
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 2})

# Plot formatting for  A
plt.legend()
plt.xlabel('Errors')
plt.ylabel('Normalized Errors (density)')
plt.title('Normal and Density Distribution of Errors')

plt.savefig('densityPlotHist.jpg', bbox_inches='tight', dpi=300)

plus_one_std_dev = np.mean(error_info.error) + np.std(error_info.error)
minus_one_std_dev = np.mean(error_info.error) - np.std(error_info.error)

import pandas as pd
import seaborn as sns
import scipy.stats as stats
import warnings
import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize = [8, 8]) 

kde = stats.gaussian_kde(error_info.error)
pos = np.linspace(min(error_info.error), max(error_info.error), 50000)
plt.plot(pos, kde(pos), color='purple')
shade = np.linspace(minus_one_std_dev, plus_one_std_dev, 300)
plt.fill_between(shade, kde(shade), alpha=0.5, color='purple',)
plt.text(x=0.25, y=.0085, horizontalalignment='center', fontsize=10, 
         s="68% of values fall within\n this shaded area of\n plus or minus 1 standard\n deviation from the mean", 
         bbox=dict(facecolor='whitesmoke', boxstyle="round, pad=0.25"))
plt.title("KDE Plot of Normal Distribution of Values", fontsize=10, y=1.012)
plt.xlabel("values", labelpad=15)
plt.ylabel("probability", labelpad=15);

plt.savefig('kdePlot.jpg', bbox_inches='tight', dpi=300)

std_dev = round(np.std(error_info.error), 1)
median = round(np.median(error_info.error), 1)

print("normal_distr_values has a median of {0} and a standard deviation of {1}".format(median, std_dev))

mean = round(np.mean(error_info.error), 1)
mean

for number_deviations in [-3, -2, -1, 1, 2, 3]:
    value = round(np.mean(error_info.error) + number_deviations * np.std(error_info.error), 1)
    print("{0} is {1} standard deviations from the mean".format(value, number_deviations))

import pandas as pd
import seaborn as sns
import scipy.stats as stats
import warnings
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('classic')

fig, ax = plt.subplots(1, 1)
x = stats.probplot(error_info.error, dist="norm", fit=True, rvalue=True, plot=plt)

ax.get_lines()[0].set_markerfacecolor('pink')
ax.get_lines()[0].set_markeredgecolor('blue')
ax.get_lines()[0].set_markersize(6)

plt.xlabel("Theoretical quantiles | Interpretation: standard deviations", labelpad=15)
plt.title("Probability Plot to Compare Normal Distribution Values to\n Perfectly Normal Distribution", y=1.015)

plt.savefig('probabilityPlot.jpg', bbox_inches='tight', dpi=300)

#Shapiro-Wilk Test to test Normal Distribution (slow way)
w, pvalue = stats.shapiro(error_info.error)
print(w, pvalue)

#Normal Q-Q plot test Normal distribution
#From the above figure, we see that all data points lie to close to the 45-degree line 
#and hence we can conclude that it follows Normal Distribution.
res = error_info.error
fig = sm.qqplot(res, line='s')
plt.show()

import scipy.stats as stats
stats.describe(error_info.error)

t, pvalue = stats.ttest_1samp(error_info.error, 0.010480123194236406)
print(t, pvalue)

"""# 6 Fit Model: Linear model | K-fold Cross Validation

## 6.1. Model with 10-fold cross-validation with all features

### Option 1
"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
#lm = LinearRegression()

# 1. evaluate the model
scores10 = cross_val_score(model, X_train, y_train, cv=10, scoring='r2')
print("Cross-validation scores: {}".format(scores10))

# 2. report performance
print("Average cross-validation score: {:.3f}".format(scores10.mean()))
print('MAE: %.3f (%.3f)' % (mean(scores10), std(scores10)))
print("Accuracy: %0.3f (+/- %0.3f)" % (scores10.mean(), scores10.std()))

#The mean score and the 95% confidence interval of the score estimate are hence given by:
print("Accuracy for 95perc confidence interval: %0.3f (+/- %0.3f)" % (scores10.mean(), scores10.std() * 2))

# 2.1 Measure for boxplots
import statistics
from scipy import stats
# Median for predicted value
median = statistics.median(scores10)

q1, q2, q3= np.percentile(scores10,[25,50,75])
# IQR which is the difference between third and first quartile
iqr = q3 - q1

# lower_bound is 15.086 and upper bound is 43.249, so anything outside of 15.086 and 43.249 is an outlier.
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr) 

print('upper_bound: %.3f' % upper_bound)
print('Third quartile (q3): %.3f' % q3)
print('Median: %.3f' % median)
print('First quartile (q1): %.3f' % q1)
#print('Median (q2): %.3f' % q2)
print('IQR: %.3f' % iqr)
print('lower_bound: %.3f' % lower_bound)

# 3. plot performance
fig = plt.figure()
fig.suptitle('Model with 10-fold cross-validation')
ax = fig.add_subplot(111)
import matplotlib.pyplot as plt
plt.style.use('classic')

fig.set_size_inches(4, 4)

medianprops = dict(linewidth=1.5, linestyle='-', color='#fc3468')
meanprops =  dict(marker='D', markerfacecolor='indianred', markersize=4.5)

plt.gca().spines['right'].set_color('#D9D8D6')
plt.gca().spines['top'].set_color('#D9D8D6')
plt.gca().spines['left'].set_color('#D9D8D6')
plt.gca().spines['bottom'].set_color('#D9D8D6')

plt.grid(color='grey', linestyle='-', linewidth=0.25)

plt.boxplot(scores10, medianprops=medianprops, meanprops=meanprops, showmeans=True)
ax.set_xticklabels('')
plt.xlabel('Linear Regression')
plt.ylabel('Accuracy Model')

# Show the grid lines as light grey lines
#plt.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)

plt.savefig('accuracy_LM.png', bbox_inches='tight', dpi=300)

"""### Option 2"""

from sklearn.model_selection import StratifiedKFold, KFold

# step-1: create a cross-validation scheme
folds = KFold(n_splits = 10, shuffle = True, random_state = 42)
#folds = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)

# step-2: specify range of hyperparameters to tune, consider all available #features
params = [{'n_features_to_select': list(range(1, len(X_list)+1))}]
params

from sklearn.feature_selection import RFE
# step-3: perform grid search
# 3.1 specify model
# Create a linear regression object
lm = linear_model.LinearRegression()
#lm = LinearRegression()

#lm.fit(X_train, y_train)
rfe = RFE(lm)

from sklearn.model_selection import GridSearchCV

model_cv = GridSearchCV(estimator = rfe, param_grid = params, scoring= 'r2', cv = folds, verbose = 1, return_train_score=True)
model_cv

# fit the model KFold=10
model_cv.fit(X_train, y_train)

# KFold=10
#ACCURACY FOR TRAINING SET:
print("Accuracy on training set: {:.3f}".format(model_cv.score(X_train, y_train)))
#ACCURACY FOR TEST SET:
print("Accuracy on test set:: {:.3f}".format(model_cv.score(X_test, y_test)))

# 4.1  Predicting the Test set results
y_pred = model_cv.predict(X_test)
y_pred

# 4.1  Predicting the Test set results
y_pred = model_cv.predict(X_test)
y_pred

#EVALUATE MODEL: R2 | KFold=10
print("R2 (explained variance): {:.3f}".format(metrics.r2_score(y_test, y_pred), 2))
print('MAE=Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE=Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE=Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

!pip install rfpimp

from sklearn.metrics import r2_score
from rfpimp import permutation_importances

def r2(rf, X_train, y_train):
    return r2_score(y_train, model_cv.predict(X_train))

perm_imp_rfpimp = permutation_importances(model_cv, X_train, y_train, r2)

perm_imp_rfpimp

"""## 6.2 Evaluating Linear Regression Models"""

from sklearn.model_selection import cross_val_score

# function to get cross validation scores
def get_cv_scores(model):
    scores = cross_val_score(model,
                             X_train,
                             y_train,
                             cv=10,
                             scoring='r2')
    
    print('CV Mean: ', np.mean(scores))
    print('STD: ', np.std(scores))
    print('\n')

"""### 6.2.1 Linear Model | Ordinary Least Squares"""

lm = LinearRegression()
# get cross val scores
get_cv_scores(lm)

"""# 7 Fit Model: Linear Regresson |  Lasso"""

from sklearn.linear_model import Lasso
# Train model with default alpha=1
lasso = Lasso(alpha=0.1).fit(X_train, y_train)
# get cross val scores
get_cv_scores(lasso)

# find optimal alpha with grid search
alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
param_grid = dict(alpha=alpha)
grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)
grid_result = grid.fit(X_train, y_train)
print('Best Score: ', grid_result.best_score_)
print('Best Params: ', grid_result.best_params_)

from sklearn.linear_model import Lasso
# Train model with default alpha=1
lasso = Lasso(alpha=0.001, max_iter=100000).fit(X_train, y_train)
# get cross val scores
get_cv_scores(lasso)

print("Training set score: {:.3f}".format(lasso.score(X_train, y_train)))
print("Test set score: {:.3f}".format(lasso.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso.coef_!=0)))

# match column names to coefficients
for coef, col in enumerate(X_train.columns):
    print(f'{col}:  {lasso.coef_[coef]}')

y_pred = lasso.predict(X_test)
y_pred

dataPred = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
dataPred

# TRAIN: Make prediction using TRAIN set
y_train_predicted = lasso.predict(X_train)
y_train_predicted

dataTrain = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_predicted})
dataTrain['residuals']=dataTrain['Actual'] - dataTrain['Predicted']
dataTrain

#EVALUATE MODEL: R2 | 
print("R2 (explained variance) Train Set: {:.3f}".format(metrics.r2_score(y_train, y_train_predicted), 2))
print("R2 (explained variance) Test set: {:.3f}".format(metrics.r2_score(y_test, y_pred), 2))
print('MAE=Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE=Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE=Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

column_names = ['aspect','planCurvature', 'profileCurvature','slope','TPI','TWI_SAGA','Dh_diffuse','Ih_direct','DEM','meanJanRain','meanJulRain','maxJanTemp','minJulTemp','SYMBOL','soil_order','BDw','CLY', 'CFG','ECD','SOC','pHw','SND','SLT']

regression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': lasso.coef_}, columns=['Feature', 'Coefficient'])

plt.figure(figsize=(14,8))
g = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')
g.set_title("Contribution of features towards dependent variable: 'siteindex' (y)", fontsize=15)
g.set_xlabel("independent variables (x)", fontsize=13)
g.set_ylabel("slope of coefficients (m)", fontsize=13)
plt.xticks(rotation=45, horizontalalignment='right')
g.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])
g.set_xticklabels(column_names)
for p in g.patches:
    g.annotate(np.round(p.get_height(),decimals=2), 
                (p.get_x()+p.get_width()/2., p.get_height()), 
                ha='center', va='center', xytext=(0, 10), 
               textcoords='offset points', fontsize=14, color='black')
    
plt.grid(which='major', linestyle='-', linewidth='0.5', color='grey')
#plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')


plt.savefig('regCoef_lassoModel_10test.png', bbox_inches='tight', dpi=300)

from sklearn.metrics import mean_squared_error, r2_score
# Model Output
# a. Intercept
print("Intercept:", lasso.intercept_)

# b. Coefficient - the slop of the line
print("Coefficients(slope of the line):", lasso.coef_)

# c. the error - the mean square error
print("Mean squared error: %.3f"% mean_squared_error(y_test,y_pred))

# d. R-square -  how well x accout for the varaince of Y
print("R-square: %.3f'" % r2_score(y_test,y_pred))

"""# 8 Fit Model Linear Reg | Normalize"""

from sklearn import preprocessing
# Get column names first
names = EDAsurvey.columns

# Create the Scaler object
scaler2 = preprocessing.MinMaxScaler()

# Fit your data on the scaler object
EDAsurvey_norm = scaler2.fit_transform(EDAsurvey)
EDAsurvey_norm = pd.DataFrame(EDAsurvey_norm, columns=names) 
EDAsurvey_norm

# A_Target variable: Labels are the values we want to predict
X = EDAsurvey_norm.drop('siteindex', axis = 1)

# B_Independent variables: features are the values that help to predict
y = EDAsurvey_norm['siteindex']#.values.reshape(-1,1)

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)

print('Training Features Shape:', X_train.shape)
print('Training Labels Shape:', X_test.shape)
print('Testing Features Shape:', y_train.shape)
print('Testing Labels Shape:', y_test.shape)

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn import linear_model

# Create a linear regression object
modelNorm = linear_model.LinearRegression()

# TRAIN: Fit the model using the training set
modelNorm.fit(X_train, y_train)

# TEST: Make prediction using test set
predictedNorm = modelNorm.predict(X_test)
predictedNorm

y_train_predictedNorm = lasso.predict(X_train)
y_train_predictedNorm

from sklearn.metrics import mean_squared_error, r2_score
# Model Output
# a. Intercept
print("Intercept:", model.intercept_)

# b. Coefficient - the slop of the line
print("Coefficients(slope of the line):", model.coef_)

# c. the error - the mean square error
print("Mean squared error: %.3f"% mean_squared_error(y_test,predictedNorm))

# d. R-square -  how well x accout for the varaince of Y
print("R-square: %.3f'" % r2_score(y_test,predictedNorm))

#ACCURACY FOR TRAINING SET:
print("Accuracy on training set: {:.3f}".format(modelNorm.score(X_train, y_train)))
#ACCURACY FOR TEST SET:
print("Accuracy on test set:: {:.3f}".format(modelNorm.score(X_test, y_test)))

#EVALUATE MODEL: R2
print("R2 (explained variance): {:.3f}".format(metrics.r2_score(y_test, predictedNorm), 2))
print('MAE=Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictedNorm))  
print('MSE=Mean Squared Error:', metrics.mean_squared_error(y_test, predictedNorm))  
print('RMSE=Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictedNorm)))

column_names = ['aspect','planCurvature', 'profileCurvature','slope','TPI','TWI_SAGA','Dh_diffuse','Ih_direct','DEM','meanJanRain','meanJulRain','maxJanTemp','minJulTemp','SYMBOL','soil_order','BDw','CLY', 'CFG','ECD','SOC','pHw','SND','SLT']

regression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': modelNorm.coef_}, columns=['Feature', 'Coefficient'])

plt.figure(figsize=(14,8))
g = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')
g.set_title("Contribution of features towards dependent variable: 'siteindex' (y)", fontsize=15)
g.set_xlabel("independent variables (x)", fontsize=13)
g.set_ylabel("slope of coefficients (m)", fontsize=13)
plt.xticks(rotation=45, horizontalalignment='right')
g.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])
g.set_xticklabels(column_names)
for p in g.patches:
    g.annotate(np.round(p.get_height(),decimals=2), 
                (p.get_x()+p.get_width()/2., p.get_height()), 
                ha='center', va='center', xytext=(0, 10), 
               textcoords='offset points', fontsize=14, color='black')
    
plt.grid(which='major', linestyle='-', linewidth='0.5', color='grey')
#plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')


plt.savefig('regCoef_linearModel_Norm.png', bbox_inches='tight', dpi=300)

y_train.reset_index(drop=True,inplace=True)
y_test.reset_index(drop=True,inplace=True)

X_train.reset_index(drop=True,inplace=True)
X_test.reset_index(drop=True,inplace=True)

## Kpi
print("R2 (explained variance): {:.3f}".format(metrics.r2_score(y_test, predictedNorm), 2))
print("MAE=Mean Absolute Error (Σ|y-pred|/n):", "{:,.0f}".format(metrics.mean_absolute_error(y_test, predictedNorm)))
print("MAPE=Mean Absolute Percentage Error (Σ(|y-pred|/y)/n):", round(np.mean(np.abs((y_test-predictedStand)/predictedNorm)), 2))
print("MSE=Mean Square Error (Σ(y-pred)^2/n):", "{:,.0f}".format(metrics.mean_squared_error(y_test, predictedNorm)))
print("RMSE=Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)):", "{:,.0f}".format(np.sqrt(metrics.mean_squared_error(y_test, predictedNorm))))

## residuals
residuals = y_test - predictedNorm
max_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)
max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))
max_true, max_pred = y_test[max_idx], predictedNorm[max_idx]
print("Max Error:", "{:,.0f}".format(max_error))

## Plot predicted vs true
fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(8,6))

from statsmodels.graphics.api import abline_plot

ax[0].scatter(predictedNorm, y_test, color="black")
abline_plot(intercept=0, slope=1, color="red", ax=ax[0])
ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label="max error")
ax[0].grid(True)
ax[0].set(xlabel="Predicted", ylabel="True", title="Predicted vs True")
ax[0].legend()

## Plot predicted vs residuals
ax[1].scatter(predictedNorm, residuals, color="red")
ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label="max error")
ax[1].grid(True)
ax[1].set(xlabel="Predicted", ylabel="Residuals", title="Predicted vs Residuals")
ax[1].hlines(y=0, xmin=np.min(predictedNorm), xmax=np.max(predictedNorm))
ax[1].legend()
plt.show()

"""# NON LINEAR DATA MODEL

# 1 Fit Model: Polynomial Regression

## 1.1 Polynomial Regressor
"""

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(X_train)
pol_reg = LinearRegression()

# Train model
pol_reg.fit(X_poly, y_train)

"""## 1.2 Predict Test Results"""

# TEST: Make prediction using TEST set
y_pred = pol_reg.predict(poly_reg.fit_transform(X_test))
y_pred

dataTest = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
dataTest['residuals']=dataTest['Actual'] - dataTest['Predicted']
dataTest

dataTest.describe()

# TRAIN: Make prediction using TRAIN set
y_train_predicted = pol_reg.predict(X_poly)
y_train_predicted

dataTrain = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_predicted})
dataTrain['residuals']=dataTrain['Actual'] - dataTrain['Predicted']
dataTrain

dataTrain.describe()

"""### 1.2.1 Plot Predicted vs Observed | Test Set"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='whitegrid')

plt.style.use('seaborn-whitegrid')

plt.figure(figsize=(10, 6))

ax = sns.regplot(x="Actual", y="Predicted", data=dataTest, label='siteindex predicted', scatter_kws = {'color': 'white', 'alpha': 0.8, 'edgecolor':'blue', 's':10}, line_kws = {'color': '#f54a19'})
ax.set_ylim(0,55)
ax.set_xlim(0,55)
ax.plot([0, 55], [0, 55], 'k--', lw=2)

ax.legend(title="Test set:", frameon=  True, loc='upper left')
#ax.legend(bbox_to_anchor =(0.85, -0.20), ncol = 4) 
plt.title('Goodness-of-fit in Validation Set',fontsize=12)

plt.savefig('actualvsPredicted_PolyReg_testSet.jpg', bbox_inches='tight', dpi=300)

ax = sns.regplot(x="Actual", y="Predicted", data=dataTest,
                 scatter_kws = {'color': 'orange', 'alpha': 0.3}, line_kws = {'color': '#f54a19'},
                 x_estimator=np.mean, logx=True)

ax.set_ylim(0,55)
ax.set_xlim(0,55)
ax.plot([0, 55], [0, 55], 'k--', lw=2)

ax = sns.regplot(x="Actual", y=y_pred, data=dataTest,
                 scatter_kws={"s": 80},
                 order=2, ci=None)

"""## 1.3 Perfomance and Validation"""

#ACCURACY FOR TRAIN SET:
print("Accuracy on train set:: {:.3f}".format(pol_reg.score(X_poly, y_train)))

# ACCURACY FOR TEST SET:
print("Accuracy on test set:: {:.3f}".format(pol_reg.score(poly_reg.fit_transform(X_test), y_test)))

print("R2 (explained variance) Train Set: {:.3f}".format(metrics.r2_score(y_train, y_train_predicted), 2))
print("R2 (explained variance) Test set: {:.3f}".format(metrics.r2_score(y_test, y_pred), 2))
print('MAE=Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('MSE=Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('RMSE=Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

"""### 1.3.1 Plot Squared Errror vs Observed"""

residSquare = np.square(dataTest['residuals'])
residSquare

plt.style.use('seaborn-whitegrid')
fig=plt.figure(figsize = [8, 6])

ax = fig.add_subplot(111)

ax.scatter(x=dataTest['Actual'], y=residSquare, label='Squared Error', c='white', alpha=0.8, edgecolors='#1b346c', s=10)
ax.set_xlabel("Observed 'site index' values") #it's a good idea to label your axes
ax.set_ylabel('Squared Error')


plt.title("Squared Error vs Observed 'site index' values")
plt.legend(title="",loc='upper right', frameon=True)
plt.savefig('SquaredError_PolyReg.png', bbox_inches='tight', dpi=300)

fig=plt.figure(figsize = [8, 6])

ax = fig.add_subplot(111)

ax.scatter(x=dataTest['Predicted'], y=residSquare, c='#f54a19', label='Squared Error')
ax.set_xlabel("Predicted 'site index' values") #it's a good idea to label your axes
ax.set_ylabel('Squared Error')

plt.title("Squared Error vs Predicted 'site index' values")
plt.legend(title="",loc='upper right', frameon=True)
plt.savefig('SquaredErrorPredicted_PolyReg.png', bbox_inches='tight', dpi=300)

"""## 1.4 Evaluation: Slope of Coefficients"""

#pol_reg.coef_

from sklearn.metrics import mean_squared_error, r2_score
# Model Output
# a. Intercept
print("Intercept:", pol_reg.intercept_)

for coef, col in enumerate(X_train.columns):
    print(f'{col}:  {pol_reg.coef_[coef]}')

pred_model = pd.DataFrame(['aspect','planCurvature','profileCurvature','slope','TPI','TWI_SAGA','Dh_diffuse','Ih_direct','DEM','meanJanRain','meanJulRain','maxJanTemp','minJulTemp','SYMBOL','soil_order','BDw','CLY','CFG','ECD','SOC','pHw','SND','SLT'])
coeff = pd.DataFrame(pol_reg.coef_)

df = pd.concat([pred_model,coeff], axis=1, join='inner')
df

# adding column name to the respective columns 
df.columns =['Features', 'Coefficients'] 
  
# displaying the DataFrame 
print(df)

df = df.sort_values(by='Coefficients', ascending=0)
df

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14,6))
sns.set(style="whitegrid")
plt.subplot(1, 1, 1) # 1 row, 2 cols, subplot 1

ax = sns.barplot(df.Features, df.Coefficients)

for p in ax.patches:
    ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()/2., p.get_height()),
                ha='left',
                va='baseline', 
                #textcoords='offset points',
                rotation='30')
                
#Rotate labels x-axis
plt.xticks(rotation=45, horizontalalignment='right')
plt.ylabel('independent variables (x)')
plt.xlabel('Coefficents')
plt.title("Contribution of features towards dependent variable: 'siteindex' (y)")


plt.savefig('polyreg_FI.png', bbox_inches='tight', dpi=300)

from sklearn.inspection import permutation_importance
r = permutation_importance(polyreg, X_test, y_test,
                          n_repeats=30,
                           random_state=0)

for i in r.importances_mean.argsort()[::-1]:
     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
       print(f"{EDAsurvey.columns[i]:<8}"
              f"{r.importances_mean[i]:.3f}"
             f" +/- {r.importances_std[i]:.3f}")

"""## 1.5 Assessing Quality of Regression Model"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(X_train)
pol_reg = LinearRegression()

# Train model
pol_reg.fit(X_poly, y_train)

cv_scores_Kfold10 = cross_val_score(pol_reg,X_poly, y_train, cv=10, scoring='r2')
print("Cross-validation scores: {}".format(cv_scores_Kfold10))

print("Average cross-validation score: {:.3f}".format(cv_scores_Kfold10.mean()))

"""# 2 Fit Model: Polynomial Regression | cv=10"""

from sklearn.model_selection import cross_val_score
# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(X_train)
pol_reg = LinearRegression()

poly10 = cross_val_score(pol_reg , X_poly, y_train, cv=10, scoring='r2')
print("Cross-validation scores: {}".format(poly10))

# evaluate adaboost algorithm for classification
from numpy import mean
from numpy import std
# 2. report performance
print("Average cross-validation score: {:.3f}".format(poly10.mean()))
print('MAE: %.3f (%.3f)' % (mean(poly10), std(poly10)))
print("Accuracy: %0.3f (+/- %0.3f)" % (poly10.mean(), poly10.std()))

#The mean score and the 95% confidence interval of the score estimate are hence given by:
print("Accuracy for 95perc confidence interval: %0.3f (+/- %0.3f)" % (poly10.mean(), poly10.std() * 2))

import statistics
from scipy import stats
# Median for predicted value
median = statistics.median(poly10)

q1, q2, q3= np.percentile(poly10,[25,50,75])
# IQR which is the difference between third and first quartile
iqr = q3 - q1

# lower_bound is 15.086 and upper bound is 43.249, so anything outside of 15.086 and 43.249 is an outlier.
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr) 

print('upper_bound: %.3f' % upper_bound)
print('Third quartile (q3): %.3f' % q3)
print('Median: %.3f' % median)
print('First quartile (q1): %.3f' % q1)
#print('Median (q2): %.3f' % q2)
print('IQR: %.3f' % iqr)
print('lower_bound: %.3f' % lower_bound)

# 3. plot performance
fig = plt.figure()
fig.suptitle('Model with 10-fold cross-validation')
ax = fig.add_subplot(111)
import matplotlib.pyplot as plt
plt.style.use('classic')

fig.set_size_inches(4, 4)
medianprops = dict(linewidth=1.5, linestyle='-', color='#fc3468')
meanprops =  dict(marker='D', markerfacecolor='indianred', markersize=4.5)

plt.gca().spines['right'].set_color('#D9D8D6')
plt.gca().spines['top'].set_color('#D9D8D6')
plt.gca().spines['left'].set_color('#D9D8D6')
plt.gca().spines['bottom'].set_color('#D9D8D6')

plt.grid(color='grey', linestyle='-', linewidth=0.25)

plt.boxplot(poly10, medianprops=medianprops, meanprops=meanprops, showmeans=True )
ax.set_xticklabels('')
plt.xlabel('Polynomial Regression')
plt.ylabel('Accuracy Model')
plt.savefig('accuracy_polyReg.png', bbox_inches='tight', dpi=300)

fig = plt.figure()
fig.suptitle('Model with 10-fold cross-validation')
ax = fig.add_subplot(111)
import seaborn as sns
sns.set(style="whitegrid")


plt.boxplot(poly10)
ax.set_xticklabels('')
plt.xlabel('Polynomial Regression')
plt.ylabel('Accuracy Model')
plt.savefig('accuracy_polyReg.png', bbox_inches='tight', dpi=300)

"""# 3 Fit Model: Polynomial Regression 2 | Testing

### 3.1 Polynomial Reg 2
"""

X = X.values.reshape(-1,1)
X

y

X

y

y

# A_Target variable: Labels are the values we want to predict
X = EDAsurvey.drop('siteindex', axis = 1)

# Saving feature names for later use
X_list = list(EDAsurvey.columns)

# B_Independent variables: features are the values that help to predict
y = EDAsurvey['siteindex'].values.reshape(-1,1)

# Using Skicit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)

print('Training Features Shape:', X_train.shape)
print('Training Labels Shape:', X_test.shape)
print('Testing Features Shape:', y_train.shape)
print('Testing Labels Shape:', y_test.shape)

# drop coordinates
EDAsurvey.drop(columns=['x', 'y'], inplace= True, axis = 1)

# Fitting Linear Regression to the dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

polyModel = PolynomialFeatures(degree = 2)
xpol = polyModel.fit_transform(X_train.values.reshape(-1, 1))
preg = polyModel.fit(xpol,y_train.reshape(-1, 1))

xpol.reshape(-1, 2)

preg

X_train

xpol

polyModel = PolynomialFeatures(degree = 2)
xpol = polyModel.fit_transform(X_train)
preg = polyModel.fit(xpol,y_train)

# Fitting Linear Regression to the dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train.values, y_train.values.reshape(-1, 1))

# Visualizing the Linear Regression results
def viz_linear():
    plt.scatter(X_train.values.reshape(-1, 1)[1:5000], y_train[1:5000], color='red')
    plt.plot(X_train[1:5000], lin_reg.predict(X_train)[1:5000], color='blue')
    plt.title('Truth or Bluff (Linear Regression)')
    plt.xlabel('Position level')
    plt.ylabel('Salary')
    plt.show()
    return
viz_linear()

lin_reg.predict(X_test)[1:5].reshape(1,-1)

dataTest = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
dataTest['residuals']=dataTest['Actual'] - dataTest['Predicted']
dataTest

"""## 3.2 Array to dataframe"""

y_pred_poly = pol_reg.predict(poly_reg.fit_transform(X_test))
y_pred_poly

y_test.values.reshape(-1,1)

index = ['' + str(i)  
        for i in range(1, len(y_pred_poly) + 1)] 

columns = ['Predicted' + str(i)  
          for i in range(1, len(y_pred_poly[0]) + 1)] 

panda_df = pd.DataFrame(y_pred_poly ,  
                        index = index, 
                        columns = columns)
panda_df

y_poly = pol_reg.predict(poly_reg.fit_transform(X_train))
y_poly

residuals=y_test.values.reshape(-1,1)- y_pred_poly
residuals

np.mean(residuals)

residuals.reshape(1,-1)

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=2)
X_poly = poly_reg.fit_transform(X_train)
pol_reg = LinearRegression()
pol_reg.fit(X_poly, y_train.values.reshape(-1, 1))

# Visualizing the Polymonial Regression results
def viz_polymonial():
    plt.scatter(X_train.values.reshape(-1, 1)[1:5], y_train[1:5], color='red')
    plt.plot(X_train[1:5], pol_reg.predict(poly_reg.fit_transform(X_train)[1:5]), color='blue')
    plt.title('Truth or Bluff (Linear Regression)')
    plt.xlabel('Position level')
    plt.ylabel('Salary')
    plt.show()
    return
viz_polymonial()

n = 250         # elements number
x = list(range(n))
x = [i/100 for i in x]

def GetPolyData(x):    
    return np.sin(x) + np.random.uniform(-.2, .2, n) 

y = GetPolyData(x)
train_x = np.array(x)
train_y = np.array(y)

plt.scatter(train_x, train_y)
plt.show()